\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{comment}

\title{EE451 Final Project Proposal: \\Predicting Optimal Thread Count in Kernel Based Image Processing}

\author{Nathan Frank, Alex Baker, Grace Susanto}
\date{March 2021}

\begin{document}

    \maketitle
    
    \section{Introduction}
        \begin{multicols}{2}
            As the resolution of images increases with constant camera and display technology advances, there is a need for faster methods of doing basic kernel-based operations such as Edge detection, sharpening, and blurring on these images as well as performing more advanced Computer Vision Algorithms faster.  This problem is meaningful because as time goes on, image sizes will only continue to increase.  In order to quickly compute these operations, efficiently employing multiple workers whether they be threads in a CPU or nodes in a network, will be necessary to keep the the processing time to an acceptable level.  However, the brute force approach will often overshoot what is required for optimal speedup.  As higher thread count processors become more commonplace as well, the fastest calculation might not be with the largest amount of threads due to the accumulation of overhead on thread creation and exit.
            
            The problem being addressed by this project is the avoidance of overhead in parallelized image processing operations.  We will be parallelizing three algorithms, a basic edge detection kernel based algorithm, a SIFT (Scale-invariant feature transform) algorithm, and finally a Moravec corner detection algorithm.  We will be attempting to avoid the overhead of having too many threads by first defining each algorithm in a scalable way and then testing it for a large variety of threads in the range of 1 - 32 threads, the range of dedicated threads on common consumer hardware that is readily available in the year 2021.  After gathering the most optimal thread counts for an average image size using this testing method we will test if the values found are optimal on a smaller selection of images that were not part of the training data.  From the training data we will then try to create an algorithm that takes the speed of the processor, size of the input data, and complexity of desired operation into account when roughly approximating what amount of threads would result in the least amount of excess overhead.
            
        \end{multicols}
        \pagebreak
    
    \section{Background}
        In one of the most popular open source raster-based image editing programs, GIMP (GNU Image Manipulation Program) support for applying these techniques to images using multiple threads is still not fully integrated.  But, the parts that are do support multi-threading are only ever run at a global user-defined thread count.  As the average amount of threads per cpu increases it will become more important that these operations are fully multi-threaded, but they are also run in a manner to reduce any excess.  The purpose of this project is to determine if training a program to avoid overhead will provide any meaningful increase to the performance.  There are not any consumer programs that function in a way that avoids unnecessary overhead.
        \pagebreak
    
    \section{Proposed Work}
        \begin{multicols}{2}
            \subsection{Focus}
                The focus of this project will be the performance and how scalable the proposed techniques are.  To evaluate this we will be running each operation for varying amounts of threads and varying image sizes with a 3x3 edge detection kernel (while any other kernel would work just as well) and updating the optimal thread-counts for each pixel count.  Then, after training the program, we will be running a group of test images through the program and logging what thread count the program chose for that image and what the actual optimal thread count is as well as how much time was saved or lost by the program choosing that thread count.
            
            \subsection{Language and Platform}
                This project will be done using C++ and POSIX threads (pthread).  We will be using this language as it is scalable, does not rely on the GPU, and can be compiled onto any target platform which falls in line with the goal of creating a generalized solution to the problem at hand.  If we were to use CUDA it would then require special hardware beyond a basic computer, it would require a NVIDIA GPU.
        
            \subsection{Technical Approach}
                We will break down the program into three main phases.  Those are pre-processing, training, and testing.
                
                Pre-processing:  For this phase the program will go through all input images and tag them with their pixel counts and which group they belong to.
                
                Training: In this phase of the program it will go through each image for all test thread counts and average the most optimal thread count into the means current most optimal thread count.  The per image per thread count execution time will also be logged in order to capture the speedup of each image for both debugging and analysis in the final report.
                
                Testing: For the testing phase of the program, a image falling into each mean data set will be processed at the optimal thread count decided by the program and then the resulting time will be compared against that images results from the training phase.  The speedup or slowdown caused by the program's optimal thread count will be logged.
                
                The program is planned to be done entirely in C++ using POSIX threads.  It will be broken into several files in order to better distribute the workload between group members.  The project is managed using GitHub with the any latex files being hosted on an overleaf project that is synced to the GitHub repository.
            
            \subsection{Group Work Division}
                \begin{itemize}
                \item Preprocessing (Nathan Frank, Alex Baker)
                    \begin{itemize}
                    \item Tag images with appropriate pixel counts (Alex)
                    \item Multi-threaded kmeans (Nathan)
                    \end{itemize}
                    
                \item Testing (Nathan Frank, Grace)
                    \begin{itemize}
                    \item Multi-threaded kernel edge detection (Nathan Frank)
                    \item kmeans updates (Grace)
                    \end{itemize}
                
                \item Thread management (Grace, Alex)
                    \begin{itemize}
                    \item partitioning data, spawning threads, running all trials
                    \end{itemize}
                    
                \item Final Report (Nathan, Alex, Grace)
                    \begin{itemize}
                    \item Making graphs (Nathan)
                    \item Writing summaries (Nathan, Alex, Grace)
                    \item Evaluation of results (Alex)
                    \end{itemize}
                    
                \end{itemize}
            
            \subsection{Evaluation}
                The results expected are smaller thread counts to be optimal for much smaller images and larger thread counts to be optimal for very large images.  We will be using the images from the  ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012-2017.  This is a set of 100,000 images that we will be grouping into different sizes using a k-means grouping with the standard resolutions of 480p, 720p, 1080p (To be changed depending on dataset averages) as the starting points with total pixel count as the metric for the grouping.  The link for the data-set is http://image-net.org/download
                
            \subsection{Challenges}
                The challenges for this project are implementing the program as listed above.  Our main goal will be to address the issues with computation of kernel based image processing on large images and computing what will produce that fastest performance for each image size.  By the end of the experiment, using the data captured we should be able to predict how many threads and what kernel size to use for a given image to produce the output as fast as possible before the computation begins.
                
            \end{multicols}
            
        \pagebreak
    
    \section{Implementation}
        The three algorithm's we will be parallelizing, the kernel based edge detection, the SIFT algorithm, and the Moravec corner detection will all require different approaches to parallelizing in an efficient and scalable manner.
        
        Starting with the kernel based edge detection algorithms, these algorithms take the image in as 3 matrices of Red, Green, and Blue values perform convolution with the kernel.  A edge detection kernel looks like the following:\\\\
        
        3x3 Edge Detection Kernel
        $\begin{pmatrix*}[r]
        -1 & -1 & -1\\
        -1 &  8 & -1\\
        -1 & -1 & -1
        \end{pmatrix*}\\\\$
        
        5x5 Edge Detection Kernel
        $\begin{pmatrix*}[r]
        0 &   0 & -1 &  0 &  0\\
        0 &  -1 & -2 & -1 &  0\\
        -1 & -2 & 16 & -2 & -1\\
        0 &  -1 & -2 & -1 &  0\\
        0 &   0 & -1 &  0 &  0
        \end{pmatrix*}\\\\$
        
        With a central value of 8 and -1 for all surrounding values the edges will be made more prominent as they will be the only remaining values due to their high local contrast.  To carry out this algorithm out on an image in serial time all you need to do is go pixel by pixel and perform convolution with the kernel at each point.  By the end, the result will be an image that only has the edges remaining.  In order to parallelize this algorithm we will be passing the kernel to each thread and dividing the input image into blocks based on the amount of threads currently being tested.  The pseudo-code for the parallelized pseudo-code is on the following pages.  It is written in a python-like implementation for readability.  One thing to note with the example matrices for the kernel is that for all intents and purposes, the actual values do not matter for this experiment.  The purpose of this experiment is to predicatively improve the performance of an algorithm, but to detect the edges of an image.
        
        The next algorithm we will be parallelizing, in order of complexity, is the Moravec corner detection algorithm.  This algorithm involves going pixel by pixel and comparing the intensity of the surrounding regions of that pixel.  In order to get the intensity readings, we will first be turning the images from full color into gray-scale by performing a conversion using the luminosity method where we multiply the red, green, and blue values by a weight and add them together to get which value between 0-256 should be the grey-scale color of that pixel.  This conversion will also be parallelized.  The formula for this conversion is as follows:\\\\
        
        \[New value = (0.3*Red + 0.59*Green + 0.11*Blue)\]\\
        
        This algorithm will also be broken into blocks with a grid a threads running over it.  By taking advantage of the Concurrent read ability of POSIX threads, the algorithm can be simplified.  The python-like psuedo-code of this algorithm is in section 4.2.  The Moravec corner detection algorithm using a small region to compare between the adjacent tiles of the same size.  For our implementation of the algorithm we will be using a 3x3 window size.  In order to find if a selected region around a pixel is a corner, we will be comparing the squared intensity differences of the intensity of the root region and one of the selected adjacent regions and finding the minimum squared intensity difference value between all adjacent regions.  If the value is lower than a threshold it is a corner.  Because we are not evaluating the output, but just the performance of the algorithm with varying thread counts, we will make our output look interesting and add a red dot to the output image wherever there is a corner.  This will not be a time part of the execution because it is not necessary.
        \pagebreak
        
        \subsection{Parallelized Edge Detection Psuedo-Code}
        \begin{lstlisting}
# Load the image in as an array of int[i][j][k]
image = load_image()
thread_count = input("ThreadCount: ")
kernel = [{-1, -1, -1,},{-1, 8, -1}, {-1, -1, -1}]

# The threads will be split into a grid and 
# assigned chunks of the image
vert_count = len(image[i])/sqrt(thread_count)
horz_count = len(image[i][j])/sqrt(thread_count)

# v is this threads vertical ID
# h is this threads horizontal ID
# image is the input image
# result is the output image
# kernel is a copy of the kernel
def kernel_function(v, h, image, result, kernel):
    h_offset = h * horz_count
    v_offset = v * vert_count
    
    for i in range(v_offset, v_offset + vert_count):
        for j in range(h_offset, h_offset + vert_count):
            for k in range(0,3):
                # Perform convolution, sum all the neighbors 
                # multiplied by their coefficients
                # Omited actual formula to save space
                result[i][j][k] = convolute(kernel, image[i][j][k])

def main():
    for i in range(0, vert_count):
        for j in range (0, horz_count):
            h_off = h * horz_count
            v_off = v * vert_count
            thread_create(i, h, 
                image[v_off:v_off+vert_count][h_off:h_off+horz_count][0:3], 
                result)
                
    for i in range(0, vert_count):
        for j in range (0, horz_count):
            join_thread(i, j)

        \end{lstlisting}
        \pagebreak       
        
        \subsection{Parallelized Moravec Corner Detection Algorithm Psuedo-Code}
                \begin{lstlisting}
# Load the image in as an array of int[i][j][k]
image = load_image()
thread_count = input("ThreadCount: ")
kernel = [{-1, -1, -1,},{-1, 8, -1}, {-1, -1, -1}]
threshold = 0.5 # Any threshold

# The threads will be split into a grid and 
# assigned chunks of the image
vert_count = len(image[i])/sqrt(thread_count)
horz_count = len(image[i][j])/sqrt(thread_count)

def window(i, j):
    intensity = (result[i][j] + 
                result[i][j-1] + 
                result[i][j+1] +
                result[i-1][j] + 
                result[i-1][j-1] + 
                result[i-1][j+1] +
                result[i+1][j] + 
                result[i+1][j-1] + 
                result[i+1][j+1]) / 9
    return intensity

def kernel_function(v, h, image, result):
    h_offset = h * horz_count
    v_offset = v * vert_count
    
    # Convert block to grayscale
    for i in range(v_offset, v_offset + vert_count):
        for j in range(h_offset, h_offset + vert_count):
            result[i][j] = 
                (0.3*image[i][j][0]+0.59*image[i][j][1]+0.11*image[i][j][2])
    
    border.arrived() # Wait till all threads have created the grayscale images
    
    for i in range(v_offset, v_offset + vert_count):
        for j in range(h_offset, h_offset + vert_count):
            neighbors = []
            sqr_int_diff = 0
            
            for u in range(-1,2):   
                for v in range(-1,2);
                    if u == 0 and v == 0:
                        pass
                    else:
                        neighbors.append(
                            pow(avg(window(i+u,j+u))-avg(window(i,j)),2))
            
            minimum = min(neighbors)
            # It is a corner make a red dot on the image
            if minimum < threshold:
                image[i][j][0] = 255
                image[i][j][1] = 0
                image[i][j][2] = 0
                    

def main():
    for i in range(0, vert_count):
        for j in range (0, horz_count):
            h_off = h * horz_count
            v_off = v * vert_count
            thread_create(i, h, 
                image[v_off:v_off+vert_count][h_off:h_off+horz_count][0:3], 
                result)
                
    for i in range(0, vert_count):
        for j in range (0, horz_count):
            join_thread(i, j)

        \end{lstlisting}
        \pagebreak
        
        The final algorithm will of course be written in C++ using POSIX threads.  The final code will also allow for dynamic kernel sizes in order to test kernels of various sizes during the program to see if changing the size of the kernel has any effect on the performance of the code
        
        The next algorithm we will be parallelizing is the SIFT algorithm which is a much more complicated algorithm
        \pagebreak
\end{document}